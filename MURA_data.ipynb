{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MURA_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ondraperny/BI-BPR-2019/blob/master/MURA_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng7YRBj2Xhoz",
        "colab_type": "text"
      },
      "source": [
        "# WIP info\n",
        "---\n",
        "Whole preprocessing takes few minutes, up to about 20 mins (for whole MURA). Since preprocessing of complete dataset is done only once, for that reason I didn't seem optimalization of this process as priority.\n",
        "For preprocessing I set two main tasks. To cut region-of-interest from original image to get rid of most of the background (usually monochromatic with X-ray descriptive signs) that does not carry relevant information. And the second task is to normalize brightness (as brightness of input images vary a lot). For both tasks I tried to use different method methods (which can be found in Data_preprocessor.ipynb, in class ImageFilters). All of them could be used, but I choose empirically (training on part of the dataset) the best performing ones. \\\n",
        "\n",
        "Preprocessing could be made on-the-fly, mostly it could require change of method that currently loads data. However my reason agains doing this is the unnecessary overhead that would be added to training model. Data can be preprocessed only once and then used as many times during training without preprocessing them everytime. Advantage to this approach is unnecessity of storing preprocessed data, however the original dataset takes just 3.14Gb space on disk and preprocessed data even less. This cannot justify the overhead increased in data model, which is I didn't choose this solution. \\\n",
        "\n",
        "ImageDataGenerator has many built-in methods that allows very easy data augmentation accesible by its parameters. But offers just basic augmentation, which is why I needed to make my own preprocessing for getting ROI and normalization of image brightness. Currently used augmentations are just what I thought could be good, based on different models that I have inspected with regards to my specific input data (plus bit of empirical testing). I will check other possible augmentations (as brightness_range, etc.). \\\n",
        "\n",
        "Comments in first VGG cell are mine own, thought as most of them are definitions it is possible that some content of comments is worded very similar or same as in other sources. As for why \"three_block_VGG\" performs better than \"simple_deep_CNN\" I can't tell for sure. My guesses are that dropout layers losing too much of significant imformations in simple deep or that conv2D layer with higher output dimension perform better on this data. \\\n",
        "\n",
        "Training for XR_HUMERUS data with 40 epochs takes about 20 minutes. \\\n",
        "All generated graph are generated directly from model (fit_generator) output, so are completely legitimate. Visuals of plot were changed a bit to be more clear now. \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU19OXW480YR",
        "colab_type": "text"
      },
      "source": [
        "# Mura dataset\n",
        "---\n",
        "### Data:\n",
        "Train and validation data for XR_HUMERUS from MURA dataset. \\\n",
        "\n",
        "---\n",
        "### Code parts(description of colab cells):\n",
        "**Imports** - contain all used imports with descriptions \\\n",
        "**Experimental** - code for experimenting with new features but is not currenly used in production version \\\n",
        "**Colab essentials** - when running code on Colab, mount Google Drive so data can be read from there. \\\n",
        "**Constants declaration** - declaring input parameters \\\n",
        "**Data augmentation** - defining functions for importing and augmenting data in Keras \\\n",
        "**Auxiliary functions** - any functions non-related to ML training, usually for graphical output \\\n",
        "**Model definitions** - Defining keras models \\\n",
        "**Training model** - initialization and training model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHZh_eZa6csV",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbw0u9Xp2wwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For graphical outpus, images preview and training results plot representation\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from tensorflow.keras import backend\n",
        "# Generator used for loading data into Keras model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Essentials for creating model architecture, model class, layers and optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import he_uniform \n",
        "# Logger for logging results from training model\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "# System functions and path processing\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Support functions for saving and loading model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPUjSwk8A3T-",
        "colab_type": "text"
      },
      "source": [
        "## Experimental\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROyBQM_uZiKp",
        "colab_type": "text"
      },
      "source": [
        "#### Experimental settings for GPU training\n",
        "Currently not working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIEX2_x8RFNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf\n",
        "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "# if gpus:\n",
        "#     try:\n",
        "#         for gpu in gpus:\n",
        "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "#     except RuntimeError as e:\n",
        "#         print(e)\n",
        "\n",
        "# from keras import backend as K\n",
        "# K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmI6V9V7aELY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(device_lib.list_local_devices())\n",
        "\n",
        "# from keras import backend as K\n",
        "# K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JDXgsebZapf",
        "colab_type": "text"
      },
      "source": [
        "#### function for replacing layers in Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjZ9Wk8eZRMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Function to replace/remove/add layer from Model (class Model, not Sequential)\n",
        "# import re\n",
        "# from keras.models import Model\n",
        "# def return_dense_2():\n",
        "#   return Dense(2, activation='softmax')\n",
        "\n",
        "# def insert_layer_nonseq(model, layer_regex, insert_layer_factory,\n",
        "#                       insert_layer_name=None, position='after'):\n",
        "\n",
        "#   # Auxiliary dictionary to describe the network graph\n",
        "#   network_dict = {'input_layers_of': {}, 'new_output_tensor_of': {}}\n",
        "\n",
        "#   # Set the input layers of each layer\n",
        "#   for layer in model.layers:\n",
        "#       for node in layer._outbound_nodes:\n",
        "#           layer_name = node.outbound_layer.name\n",
        "#           if layer_name not in network_dict['input_layers_of']:\n",
        "#               network_dict['input_layers_of'].update(\n",
        "#                       {layer_name: [layer.name]})\n",
        "#           else:\n",
        "#               network_dict['input_layers_of'][layer_name].append(layer.name)\n",
        "\n",
        "#   # Set the output tensor of the input layer\n",
        "#   network_dict['new_output_tensor_of'].update(\n",
        "#           {model.layers[0].name: model.input})\n",
        "\n",
        "#   # Iterate over all layers after the input\n",
        "#   for layer in model.layers[1:]:\n",
        "\n",
        "#       # Determine input tensors\n",
        "#       layer_input = [network_dict['new_output_tensor_of'][layer_aux] \n",
        "#               for layer_aux in network_dict['input_layers_of'][layer.name]]\n",
        "#       if len(layer_input) == 1:\n",
        "#           layer_input = layer_input[0]\n",
        "\n",
        "#       # Insert layer if name matches the regular expression\n",
        "#       if re.match(layer_regex, layer.name):\n",
        "#           if position == 'replace':\n",
        "#               x = layer_input\n",
        "#           elif position == 'after':\n",
        "#               x = layer(layer_input)\n",
        "#           elif position == 'before':\n",
        "#               pass\n",
        "#           else:\n",
        "#               raise ValueError('position must be: before, after or replace')\n",
        "\n",
        "#           new_layer = insert_layer_factory()\n",
        "#           if insert_layer_name:\n",
        "#               new_layer.name = insert_layer_name\n",
        "#           else:\n",
        "#               new_layer.name = '{}_{}'.format(layer.name, \n",
        "#                                               new_layer.name)\n",
        "#           x = new_layer(x)\n",
        "#           print('Layer {} inserted after layer {}'.format(new_layer.name,\n",
        "#                                                           layer.name))\n",
        "#           if position == 'before':\n",
        "#               x = layer(x)\n",
        "#       else:\n",
        "#           x = layer(layer_input)\n",
        "\n",
        "#       # Set new output tensor (the original one, or the one of the inserted\n",
        "#       # layer)\n",
        "#       network_dict['new_output_tensor_of'].update({layer.name: x})\n",
        "\n",
        "#   return Model(inputs=model.inputs, outputs=x)\n",
        "\n",
        "# densenet_model = insert_layer_nonseq(densenet_model, 'fc1000', return_dense_2, insert_layer_name='Dense_2', position='replace')\n",
        "# densenet_model.compile(optimizer='adam',\n",
        "#             loss='sparse_categorical_crossentropy',\n",
        "#             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj77E8dk-gzD",
        "colab_type": "text"
      },
      "source": [
        "## Colab essentials\n",
        "Running code on Colab and local requires slightly different, prerequisites (as different paths, etc.)\n",
        "For differentiating where code is run, variable IN_COLAB is used, if True then run is in Colab (so rest of code can reflect that)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ou730Gx-ljc",
        "colab_type": "code",
        "outputId": "eccc3167-d62b-44fc-a0a2-27c33cbb5951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# check if code run on colab or local, if in Colab then True\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# if run in Colab, then mount to run Google Drive file system\n",
        "if IN_COLAB:\n",
        "  # Mount google drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDeg8JQj5Ehf",
        "colab_type": "text"
      },
      "source": [
        "##  Constants declaration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQP0JUM5cNCv",
        "colab_type": "text"
      },
      "source": [
        "##### Constants that are expected to be directly changed (input parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt_lzmQQ5DDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_ORIGINAL_DATA=\"/content/drive/My Drive/SKOLA/Bachelor_work/XR_HUMERUS/\"\n",
        "COMMON_PATH = \"/content/drive/My Drive/SKOLA/Bachelor_work/preprocessed_data/\"\n",
        "\n",
        "PATH_DESTINATION_CROPPED_RECT = COMMON_PATH + \"cropped_rect/\"\n",
        "PATH_DESTINATION_CROPPED_SQUARE = COMMON_PATH + \"cropped_square/\"\n",
        "PATH_DESTINATION_HIST_EQ = COMMON_PATH + \"hist_eq/\"\n",
        "PATH_DESTINATION_CLAHE = COMMON_PATH + \"clahe/\"\n",
        "# cropped_rect_clahe shows so far best results, currently used\n",
        "PATH_DESTINATION_CROPPED_RECT_CLAHE = COMMON_PATH + \"cropped_rect_clahe/\"\n",
        "PATH_DESTINATION_CROPPED_SQUARE_CLAHE = COMMON_PATH + \"cropped_square_clahe/\"\n",
        "\n",
        "# Path to file with input data (and where outputs will be saved)\n",
        "PATH = PATH_DESTINATION_CROPPED_RECT_CLAHE\n",
        "\n",
        "IMG_SIZE=(224,224)\n",
        "INPUT_SHAPE = (*IMG_SIZE, 3)\n",
        "BATCH_SIZE = 64\n",
        "NUMBER_CLASSES = 2\n",
        "NUMBER_EPOCHS = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSD3vBDPbapr",
        "colab_type": "text"
      },
      "source": [
        "##### Deriving other constants from given values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQEZEgFIam0D",
        "colab_type": "code",
        "outputId": "b56721bd-f64c-414d-c737-52be48ab0fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def files_number(PATH):\n",
        "  \"\"\"find number of files(recursively) in given directory(path)\"\"\"\n",
        "  total = 0\n",
        "  for root, dirs, files in os.walk(PATH):\n",
        "      total += len(files)\n",
        "  return total\n",
        "\n",
        "# if run on local machine, change path to correspond my local system paths\n",
        "if not IN_COLAB:\n",
        "  PATH = \"G:\"+PATH[14:].replace('/', '\\\\\\\\')\n",
        "\n",
        "PATH_TRAIN = PATH + 'train'\n",
        "PATH_VALID = PATH + 'valid'\n",
        "# calculate number of steps per epoch and validation steps from input values\n",
        "# and constants\n",
        "train_img_num = files_number(PATH_TRAIN)\n",
        "valid_img_num = files_number(PATH_VALID)\n",
        "NUMBER_STEPS_PER_EPOCH = train_img_num // BATCH_SIZE\n",
        "NUMBER_VALIDATION_STEPS = valid_img_num // BATCH_SIZE\n",
        "\n",
        "print(\"Number of steps per epoch:\", NUMBER_STEPS_PER_EPOCH)\n",
        "print(\"Number of validation steps:\", NUMBER_VALIDATION_STEPS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of steps per epoch: 19\n",
            "Number of validation steps: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyaNFRPd87u9",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation\n",
        "Currently image augmentation will be done by parameters of ImageDataGenerator.\n",
        "If in future this solution will be insufficient, I will change it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0SF2y318g9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ImageDataGenerator_def():\n",
        "  datagen = ImageDataGenerator(\n",
        "    # featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    # samplewise_center=False,  # set each sample mean to 0\n",
        "    # featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    # samplewise_std_normalization=False,  # divide each input by its std\n",
        "    # zca_whitening=False,  # apply ZCA whitening\n",
        "    # zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "    rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    # randomly shift images horizontally (fraction of total width)\n",
        "    width_shift_range=0.1,\n",
        "    # randomly shift images vertically` (fraction of total height)\n",
        "    height_shift_range=0.1,\n",
        "    # shear_range=0.05,  # set range for random shear\n",
        "    zoom_range=0.05,  # set range for random zoom\n",
        "    # channel_shift_range=0.,  # set range for random channel shifts\n",
        "    # # set mode for filling points outside the input boundaries\n",
        "    # fill_mode='nearest',\n",
        "    # cval=0.,  # value used for fill_mode = \"constant\"\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    # vertical_flip=False,  # randomly flip images\n",
        "    # set rescaling factor (applied before any other transformation)\n",
        "    rescale=1. / 255,\n",
        "    # # set function that will be applied on each input\n",
        "    # preprocessing_function=None,\n",
        "    # # image data format, either \"channels_first\" or \"channels_last\"\n",
        "    # data_format=None,\n",
        "    # # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "    # validation_split=0.0\n",
        "    brightness_range=[0.7,1.2]\n",
        "  )\n",
        "  return datagen\n",
        "\n",
        "def load_from_directory(dir_path, data_generator):\n",
        "  '''Load images from directory while transforming data (based on parameters),\n",
        "  contain other parameters for data augmentation'''\n",
        "  batches = data_generator.flow_from_directory(\n",
        "    # path to target directory from which data will be loaded\n",
        "    dir_path,\n",
        "    # resize all input images to IMG_SIZE\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    # class_mode='categorical',\n",
        "    # color_mode='grayscale'\n",
        "  )\n",
        "  return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-fmS0tr9WU7",
        "colab_type": "code",
        "outputId": "47e19343-4170-4898-c37c-c19adad0b7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "data_generator = ImageDataGenerator_def()\n",
        "\n",
        "train_batches = load_from_directory(PATH_TRAIN, data_generator)\n",
        "valid_batches = load_from_directory(PATH_VALID, ImageDataGenerator(rescale=1. / 255)\n",
        ")\n",
        "\n",
        "print(\"Found indices: \", end='')\n",
        "print(train_batches.class_indices)\n",
        "# print(train_batches.class_indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1271 images belonging to 2 classes.\n",
            "Found 287 images belonging to 2 classes.\n",
            "Found indices: {'train_negative': 0, 'train_positive': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMbozrLcV9AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def preload_data(path_to_data, batch_generator):\n",
        "#   limit = files_number(path_to_data)\n",
        "#   cnt = 0\n",
        "#   images = []\n",
        "#   labels = []\n",
        "#   while True:\n",
        "#     tmp_images, tmp_labels = batch_generator.next()\n",
        "#     for i in range(len(tmp_images)):\n",
        "#       if cnt > limit:\n",
        "#         return images, labels \n",
        "#       images.append(tmp_images[i])\n",
        "#       labels.append(tmp_labels[i])\n",
        "#       cnt += 1\n",
        "#       if cnt%100 == 0:\n",
        "#         print(cnt)\n",
        "\n",
        "# train_images, train_labels = preload_data(PATH_TRAIN, train_batches)\n",
        "# valid_images, valid_labels = preload_data(PATH_VALID, valid_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1v5wG3o8yNc",
        "colab_type": "text"
      },
      "source": [
        "## Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dBmWDHA8xOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_to_string(label):\n",
        "  '''Map label value to descriptive string'''\n",
        "  if(label == 0):\n",
        "    return \"Negative\"\n",
        "  else:\n",
        "    return \"Positive\"\n",
        "\n",
        "\n",
        "def show_sample_images(batches):\n",
        "  '''Show one batch of training images (max 25 images)'''\n",
        "  image_batch, label_batch = batches.next()\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(min(len(image_batch), 25)):\n",
        "    ax = plt.subplot(5,5,n+1)\n",
        "    plt.imshow(image_batch[n][:,:,0], cmap=cm.gray)\n",
        "    plt.title(label_to_string(label_batch[n]))\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def load_keras_model():\n",
        "  '''Return loaded model saved model, based on value of PATH'''\n",
        "  return load_model(PATH+\"latest_model.h5\")\n",
        "\n",
        "\n",
        "def plot_model_graph():\n",
        "  '''Print to output and save to file the graph of model layers\n",
        "  and its parameters, based on value of PATH'''\n",
        "  plot_model(loaded_model, to_file=PATH+\"model_arch.png\", show_shapes=True)\n",
        "\n",
        "\n",
        "def plot_training(history):\n",
        "  '''Plot diagnostic learning curves, print them to output and save them\n",
        "  to file'''\n",
        "  # plot loss\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.subplot(211)\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.ylim(bottom=0.4, top=0.8)\n",
        "  plt.xlim(left=0, right = NUMBER_EPOCHS - 1)\n",
        "  \n",
        "  # plot accuracy\n",
        "  plt.subplot(212)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.ylim(bottom=0.5, top=1)\n",
        "  plt.xlim(left=0, right = NUMBER_EPOCHS - 1)\n",
        "\n",
        "  plt.tight_layout(pad=1.0)\n",
        "  # save plot to file\n",
        "  plt.savefig(PATH + 'acc_loss_plot.png')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def cohen_kappa_metric(model_output, expected_output):\n",
        "  '''calculate cohen kappa metric between model's output and expected output \n",
        "  (golden standard defined in MURA dataset paper) \n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  model_output : List\n",
        "    a list of zeroes and ones as predicted results from model (zero for negative)\n",
        "  expected_output : List\n",
        "    a list of zeroes and ones as reported results for testing data\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Float\n",
        "    Cohen kappa metric score value\n",
        "  '''\n",
        "  from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "  if len(model_output) != len(expected_output):\n",
        "    print(\"Input strings for Kappa metrics don't have the same size.\\\n",
        "    Can't be compared\")\n",
        "    return 0.0\n",
        "  else:\n",
        "    return cohen_kappa_score(model_output, expected_output)\n",
        "\n",
        "def predict(model, imgs):\n",
        "  '''Use model to predict whether imgs are positive or negative\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : tensorflow.python.keras.engine.sequential.Sequential\n",
        "    model used for prediction\n",
        "  imgs: List\n",
        "    list of images that we want to predict on\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  List\n",
        "    A list list of zeroes and ones as prediction results (one for positive and\n",
        "    zero for negative result)\n",
        "  '''\n",
        "  # TODO\n",
        "\n",
        "  # for img in imgs:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7QB-EFKyiD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show_sample_images(train_batches)\n",
        "# show_sample_images(valid_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WznMKC7APFCX",
        "colab_type": "text"
      },
      "source": [
        "## Model definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fheoeCOuqW0e",
        "colab_type": "text"
      },
      "source": [
        "#### Currently used model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYmbTrOnq3iL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Activation function is method that define the output of neuron, different\n",
        "functions use distinctive methods of how to figure out output.\n",
        "I am using ReLU - Rectified linear unit. It is simple but effective function,\n",
        "x = max(0,x)'''\n",
        "ACTIVATION_FUNCTION='relu'\n",
        "\n",
        "'''Kernel initializer define method used in setting the initial random weights\n",
        "in layers. \n",
        "Currently using he_uniform, draws samples from a uniform distribution within\n",
        "[-limit, limit] where limit is sqrt(6 / fan_in) where fan_in\n",
        "is the number of input units in the weight ten'''\n",
        "KERNER_INITIALIZER=he_uniform(0)\n",
        "\n",
        "'''\n",
        "Padding is a method to extend input image, so filter kernel can work as intended\n",
        "even around borders.\n",
        "Same - is method that guarantee output have SAME spatial dimensions as input'''\n",
        "PADDING='same'\n",
        "\n",
        "'''Optimizer is algorithm that change the attributes of neural network\n",
        " (weights and learning rate) in order to reduce loss value\n",
        " Stochastic gradient descent, which is improved by momentum method,\n",
        " lr is learning rate - it determines how big changes can be made in each step\n",
        " during training model, bigger lr means faster learning, but step too big can\n",
        " miss the best solution, thats why i can't be way to big'''\n",
        "OPTIMIZER=SGD(lr=0.01, momentum=0.9)\n",
        "\n",
        "'''Loss function evaluate loss/cost of specific event, in this case \n",
        "loss function describe how well does model perform by comparing ground truth(\n",
        "label of what class is the training input image) with output from model'''\n",
        "LOSS_FUNCTION='binary_crossentropy'\n",
        "\n",
        "def three_block_VGG():\n",
        "  \"\"\"Three Block VGG Model\"\"\"\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation=ACTIVATION_FUNCTION, kernel_initializer=KERNER_INITIALIZER, padding=PADDING, input_shape=INPUT_SHAPE))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation=ACTIVATION_FUNCTION, kernel_initializer=KERNER_INITIALIZER, padding=PADDING))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), activation=ACTIVATION_FUNCTION, kernel_initializer=KERNER_INITIALIZER, padding=PADDING))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation=ACTIVATION_FUNCTION, kernel_initializer=KERNER_INITIALIZER))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile model\n",
        "  model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8ES5Q0crvYK",
        "colab_type": "text"
      },
      "source": [
        "#### Other models (not currently used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w7QjO3FPJuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_model():\n",
        "  \"\"\"Simple model, can't properly distinguish positive and negative images\"\"\"\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=INPUT_SHAPE))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='softmax'))\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def simple_deep_CNN():\n",
        "  \"\"\"Simple deeper model, can distuingish positive and negative images, \n",
        "  however with poor results\"\"\"\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=INPUT_SHAPE))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  \n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(NUMBER_CLASSES))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def VGG16():\n",
        "  '''Keras implentation of VGG16'''\n",
        "  vgg16 = keras.applications.vgg16.VGG16()\n",
        "  model = Sequential()\n",
        "  for layer in vgg16.layers:\n",
        "    model.add(layer)\n",
        "  model.layers.pop()\n",
        "  # for layer in model.layers:\n",
        "  #   layer.trainable = False\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def densenet():\n",
        "  \"\"\"Pre-trained Densenet201 from Keras library, with added layers to fit\n",
        "  my classification problem\"\"\"\n",
        "  densenet_model = keras.applications.densenet.DenseNet201(input_shape=INPUT_SHAPE,\n",
        "                                              include_top=False, \n",
        "                                                weights='imagenet')\n",
        "  densenet_model.trainable=False\n",
        "\n",
        "  new_densenet = Sequential([\n",
        "    densenet_model,\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(2048, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    BatchNormalization(),\n",
        "    Dense(2, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  new_densenet.compile(optimizer=Adam(lr=0.001), \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "  return new_densenet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtzQtgmvr1xN",
        "colab_type": "text"
      },
      "source": [
        "## Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlF3BkGpcA8M",
        "colab_type": "code",
        "outputId": "2f3fbce5-672c-43d3-df7a-8f58324111dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "source": [
        "# initialize model\n",
        "model = three_block_VGG()\n",
        "\n",
        "# print summary info about model and its layers\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 224, 224, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 112, 112, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 56, 56, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               12845184  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 12,938,561\n",
            "Trainable params: 12,938,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLu4W0VHdw8b",
        "colab_type": "code",
        "outputId": "44a57c27-0d86-428b-8197-f5ae2c8752cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "csv_logger = CSVLogger(PATH+'augmented_log.csv', append=True, separator=';')\n",
        "\n",
        "# train model, as input takes training data, validation data\n",
        "# steps per epoch, validations steps per epoch and number of epochs\n",
        "# callbacks is used for logging output to file\n",
        "# arguments that can be used later \n",
        "#   class_weight - if one of classes is underrepresented\n",
        "#   use_multiprocessing - to modify processing\n",
        "history = model.fit_generator(train_batches,\n",
        "                    steps_per_epoch= NUMBER_STEPS_PER_EPOCH,\n",
        "                    validation_data = valid_batches,\n",
        "                    validation_steps = NUMBER_VALIDATION_STEPS,\n",
        "                    epochs = NUMBER_EPOCHS,\n",
        "                    callbacks=[csv_logger])\n",
        "\n",
        "# Save created model\n",
        "model.save(PATH+\"latest_model.h5\")\n",
        "\n",
        "# show plots of loss/val_loss and accuracy/val_accuracy\n",
        "plot_training(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-3e861bd60418>:14: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/40\n",
            "17/19 [=========================>....] - ETA: 1:01 - loss: 1.0993 - accuracy: 0.5285"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT7VvH5nkjQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = load_keras_model()\n",
        "\n",
        "# from keras.preprocessing.image import img_to_array, load_img\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "\n",
        "# PATH_TEST = PATH+\"train/test\"\n",
        "\n",
        "# test_batches = load_from_directory(PATH_TEST, ImageDataGenerator(rescale=1. / 255))\n",
        "\n",
        "\n",
        "# # prediction = model.predict_proba(inputarray)\n",
        "# # inputarray.shape\n",
        "\n",
        "# # model1.predict_classes(inputarray)\n",
        "# model.predict_classes(test_batches, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQkiZGsRnkcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "\n",
        "# model.fit(train_images, train_labels,\n",
        "#           steps_per_epoch=32\n",
        "#           epochs=40,\n",
        "#           validation_data=(valid_images, valid_labels),\n",
        "#           )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}